{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c77cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./my_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e920ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.master('local[1]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f526584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('jupyter_logger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdbe68",
   "metadata": {},
   "source": [
    "# Построение архитектуры проекта при работе с PySpark\n",
    "\n",
    "За свою пока непродолжительную карьеру в аналитике я столкнулся со следующим типом эволюции проектов: <<ковыряние>> Excel из простых SQL запросов -> идея -> MVP -> проверка -> масштабирование. На каждом из этапов результат нужен +100500 и ещё вчера, поэтому задачами построения архитектуры проекта занимаются в последнюю очередь, если вообще занимаются. <<Рабочий код важнее ~~архитектуры~~ документации>> и в целом это может быть вполне оправдано, если люди, которые пилят этот продукт, не меняются на протяжении его существования и доживают до стабильной работы проекта после масштабирования. Но учитывая, что MVP обычно учитывает минимум пограничных случаев и особенных кейсов, да и стабильной работы от него никто не ждёт, то на посделнем этапе заканчивается забористая аналитика и начинается выстраивание стаблиных ETL процессов и добавление ~~костылей~~ доработок для учёта тех самых <<особенных>> случаев. На таком этапе может появиться потребность в доборе специалистов или искать замену приунывшим товарищам. Здесь хочу отметить, что я верю в лучший мир и лучшую организацию проектов, но пока просто с ней не сталкивался, и хочу поделиться опытом работы именно в таких реалиях.\n",
    "\n",
    "Именно в такой проект я и попал. До MVP он дожил на Excel и pandas, после первого этапа раскатки готовые Excel таблички переродились в витиеватую логику ETL процессов, написанных на помеси SQL запросов и pyspark. В этот момент я и пришёл туда новоиспечённым аналитиком. Думаю не стоит тратить много времени на эмоции, которые меня накрыли, когда меня посадили разбираться с логикой работы продукта посредствам чтения кода (помним, что работающий код важнее документации) больше похожего на наушники, которые были аккуратно сложены в карман месяц назад. На моменте, когда у меня было открыто около 7 вкладок с кодом, чтобы разобраться в принципе работы одной из функций, я понял, что с учётом постоянных доработок у меня есть большие шансы не усвоить логику работы никогда. Благо, в проекте было достаточно задач, которые можно было делать надстройками над основной логикой, куда я успешно и переключился. Но пережитая боль не давала мне покоя и я начал задумываться, каким я хотел бы видеть этот проект со структурной точки зрения?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205879e",
   "metadata": {},
   "source": [
    "## Что не так?\n",
    "\n",
    "Для того, чтобы понять, как должен выглядеть проект, я начал структурировать основные проблемы существующего. Они у меня получились следующими.\n",
    "\n",
    "* Документация. Здесь речь идёт даже не о банальных правилах расставления комментариев и описаний функций. Они, конечно, важны, намого проще разобраться в коде, если там есть хоть какие-то пояснения, но есть и другая проблема. Продукт находится в постоянном контакте с бизнесом. Логика его работы должна быть этому бизнесу понятна и оставаться актуальной. Даже если аккуратно описать все функции, сгенерировать какой-нибудь [pydoc](https://docs.python.org/3/library/pydoc.html) или [sphinx](https://www.sphinx-doc.org/en/master/), то вряд ли вы порадуете категорийных менеджеров, если дадите им это почитать. Можно, конечно, написать им и другое описание проекта, но тогда при любом изменении логики (а они происходили постоянно), придётся поменять описание внутри кода, поменять описание функции, а затем ещё и переписать документацию для бизнеса. Получается вывод, что хочется иметь такую документацию, которая была бы понятна и людям, работающим с кодом и бизнесу. А ещё неплохо бы тратить на неё не больше времени, чем на написание кода.\n",
    "\n",
    "* Нелинейность кода. Здесь, с одной стороны вспоминаем, ситуацию с 7 файлами для понимания логики работы одной функции. С другой стороны, возвращаемся к вопросу документации и объяснения этой логики. Чем больше вложенности, тем восстановить логику в один текст.\n",
    "\n",
    "* Недекомпозированность кода. Работа датафреймами в pyspark ни в коем разе не останавливает полёт фантазии. Если у вас достаточно запала то написание функции можно не останавливать никогда. ```df = select.join.withColumn.withColumn.groupby.agg.select.withColumnRenamed...``` и дальше, дальше пока не оставят силы. Можно отдохнуть. А на следующий день ```df_new = df.select.join.withColumn...```. В общем если без шуток, то я натыкался на файлы в несколько тысяч строк вот такого кода, в котором уже на 500 строчке забываешь с чего, собственно всё начиналось. Ещё одной большой проблемой длинного спарковского кода лично для меня является то, что очень быстро перестаёшь осознавать, какие вообще колонки есть в том или ином датафрейме.\n",
    "\n",
    "* Сложность тестирования. В основном, функции содержали обращения к таблицам базы внутри себя. В таких условиях очень тяжело их хоть как-то тестировать, так как тесты на больших таблицах занимают очень много времени, а при отсутствии доступа к базе вообще невозможны, а хотелось бы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2071451",
   "metadata": {},
   "source": [
    "## Структура проекта\n",
    "\n",
    "Не буду навязывать своё видение, проекты разные, задачи разные, структуры разные. Здесь просто опишу минимальную структуру, которая хорошо прижилась в моём проекте, и в контексте которой я буду дальше вести повествование. Возьмём только минимально необходимое:\n",
    "\n",
    "```\n",
    "my_project:\n",
    "|--utils\n",
    "|  |--description_builder.py\n",
    "|--modules\n",
    "|  |--__init__.py\n",
    "|  |--module_base.py\n",
    "|  |--config_base.py\n",
    "|  |--module1\n",
    "|     |--__init__.py\n",
    "|     |--config.py\n",
    "|     |--module.py\n",
    "|     |--runner.py\n",
    "|--config.yml\n",
    "|--config_sources.yml\n",
    "```\n",
    "\n",
    "На самом верхнем уровне у нас лежат конфиги проекта. У нас было всего 2 файла, поэтому в корне они никому не мешались. Если их больше, то, конечно, можно уместить их в отдельную директорию. Дальше директория со скриптами-утилитами и, собственно, дерево всех модулей проекта.\n",
    "\n",
    "Модули проекта -- его логические части. Здесь изображена структура модулей одного уровня вложенности~~, потому что я устал рисовать~~ для простоты восприятия, но уровней может быть и больше. Главное, чтобы они были логически связаны. В каждом модуле лежит файл с конфигом этого модуля, файл в котором содержатся все вычисления (module.py) и скрипт для запуска (runner.py), в котором должно быть минимум кода. Файлы module_base.py и config_base.py содержат базовые классы, речь о которых пойдёт далее. Да, хранить базовые классы именно так не самое удачное решение, но оно работает, никого не беспокоит, поэтому пусть полежат здесь.\n",
    "\n",
    "Теперь, когда мы разобрались с основой структуры, можем переходить, собственно, к самому решению, которое, на мой взгляд, сделало жизнь в проекте намного проще. Я не ставил себе задачу описать подробно весь код и функции базовых классов, но лишь показать основные принципы работы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a932f8e",
   "metadata": {},
   "source": [
    "\n",
    "## Config\n",
    "\n",
    "Начнём с конфигураций проекта. В большинстве случаев, это словарь, полученный при чтении yaml, json или подобного файла, где задана конфигурация проекта. В нашем случае это два yaml-файла. `config.yml` -- файл, содержащий параметры, общие для всего проекта. Обычно таких не очень много, в нашем случае это были сопоставления городов и макрорегионов. `config_sources.yml` -- файл, содержащий список витрин, которые используются в проекте. Источники могут быть внешними (считаются другими продуктами) и внутренними (считаются внутри проекта). Также, под проект может быть выделено несколько баз (для прода, теста, бэкапов и т.д.) и всю эту информацию нужно как-то хранить. В свойм проекте мы пришли к следующей схеме описания таблиц:\n",
    "\n",
    "```yaml\n",
    "# Внешние источники\n",
    "goods_tbl: 'edw_db.goods_table' # справочник товаров (ссылка)\n",
    "sales_tbl: 'edw_db.sales_table' # продажи магазинов (ссылка)\n",
    "\n",
    "# Базы данных продукта\n",
    "db_backups:\n",
    "    test: 'product_test_db' # база, в куторую сохраняются бэкапы расчётов\n",
    "    prod: 'product_prod_db' # база, в куторую сохраняются расчёты\n",
    "\n",
    "# Бэкапы\n",
    "backups: # витрины продукта, слепок которых делается в тестовую базу\n",
    "    product_table_1: # алиас таблицы\n",
    "        table_name: table_name_in_database # имя таблицы в базе\n",
    "        partitionedby: ['date', 'city'] # партиции\n",
    "```\n",
    "\n",
    "Сначала идёт перечисление внешних источников, которые мы просто используем и никак не меняем. Затем список баз данных продукта, в нашем случае это продовая и тестовая базы. Затем идёт описание таблиц, расчёт которых производится в проекте. Здесь полезной дополнительной информацией будет список колонок, по которым производится партицирование таблицы при записи.\n",
    "\n",
    "Помимо статических конфигов, в большинстве случаев, используются динамические, которые считываются при запуске скрипта из bash. В силу этого, каждый скрипт запуска начинается с добавления параметров в `argparse`, причём обычно, всё это полотно задаётся кописастом из предыдущего загрузчика. В силу этого, такие параметры тоже хотелось бы хранить в нашем конфиге.\n",
    "\n",
    "Ну и наконец, во время работы скриптов может понадобиться передавать некоторую информацию из функции в функцию. Конфиг, скорее всего будет тем объектом, который передаётся в каждую функцию и передавать параметры через него достаточно удобно.\n",
    "\n",
    "Итого, конфиг в проекте будет объектом класса в котором существуют 4 атрибута:\n",
    "* cfg -- для статических конфигов из config.yml (считывается при инициализации объекта класса)\n",
    "* cfg_sources -- для хранения информации по базам данных из config_sources.yml (считывается при инициализации объекта класса)\n",
    "* parameters -- параметры, получающие данные при помощи argparse. Описание параметров, которые используются повсеместно (таких как, например, TEST -- флаг использования тестовой базы, или дата расчёта), можно задавать в `__init__` базового класса\n",
    "* tmp -- дополнительные данные, которые могут быть добавлены в конфиг в процессе вычислений\n",
    "\n",
    "А также несколько полезных функций:\n",
    "* getitem, setitem -- для того, чтобы к классу можно было обращаться, как к словарю\n",
    "* print_description -- для выведения описания всех конфигураций\n",
    "* add_parameter, parse_arguments -- для добавления параметров и парсинга их значений из bash\n",
    "* get_table_link -- функция, выдающая полный путь к таблице по её алиасу\n",
    "\n",
    "В конфиг можно также накидать и других функций, результат которых зависит от статических или динамических параметров, и которые используются повсеместно в проекте.\n",
    "\n",
    "Ниже пример использования конфига для получения ссылки на таблицу `product_table_1` из описания выше. В конфиге по-умолчанию, добавлен параметр `TEST`, который определяет базу данных проекта для проведения вычислений. Также продемонстрировано, как работать с параметрами и временными значениями конфига."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2344cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test parameter 'False' table link is 'product_prod_db.table_name_in_database'\n",
      "For test parameter 'True' table link is 'product_test_db.table_name_in_database'\n",
      "---------------------------------------------------\n",
      "global_config contains:\n",
      "\n",
      "config_parameter_1 : 12\n",
      "config_parameter_2 : \n",
      "\tsubparam1 : param\n",
      "\tsubparam2 : noparam\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "sources_config contains:\n",
      "\n",
      "goods_tbl : edw_db.goods_table\n",
      "sales_tbl : edw_db.sales_table\n",
      "db_backups : \n",
      "\ttest : product_test_db\n",
      "\tprod : product_prod_db\n",
      "backups : \n",
      "\tproduct_table_1 : \n",
      "\t\ttable_name : table_name_in_database\n",
      "\t\tpartitionedby : ['date', 'city']\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "parameters contain:\n",
      "\n",
      "TEST : True\n",
      "calc_date : 12.08.2021\n",
      "logging_level : 10\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "temporary data contains:\n",
      "\n",
      "tmp_parameter : None\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from modules.config_base import ConfigBase\n",
    "# Пример вывода ссылки на таблицу\n",
    "config = ConfigBase()\n",
    "print(f\"For test parameter '{config['TEST']}' table link is '{config.get_table_link('product_table_1')}'\")\n",
    "config.update_config({'TEST': True})\n",
    "print(f\"For test parameter '{config['TEST']}' table link is '{config.get_table_link('product_table_1')}'\")\n",
    "\n",
    "# Добавление параметров\n",
    "config.add_parameter('logging_level', '--log_lev', type=int, default=10, nargs='?', help='Log level')\n",
    "config['tmp_parameter'] = None\n",
    "config.print_description()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4625b40",
   "metadata": {},
   "source": [
    "Даже такая простая функция может существенно сократить количество однотипного кода, упростить его читаемость и быть чуть более уверенным в том, что какая-либо продовая витрина не будет перезаписана случайно. На функцию печати содержания конфига тоже рекомендую потратить время, это экономит много времени в дальнейшем, когда модулей в проекте становится больше, чем один.\n",
    "\n",
    "В заключении следует оговорить, что же тогда содержится в файлах `config.py` каждого отдельного модуля? В этих файлах происходит инициализация объекта базового класса `ConfigBase` и добавление в этот объект параметров, имеющих отношение к модулю (функция `add_parameter()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb1b11",
   "metadata": {},
   "source": [
    "## Линеаризация структуры\n",
    "\n",
    "Иметь хороший конфиг, конечно, приятно, но [глобальных проблем](#Каким-же-должен-быть-код?) это не решает. На мой взгляд, основными проблемами, мешающими нормально воспринимать код, составлять по нему документацию и тестировать его, являлись непоследовательность его написания и отсутствие декомпозиции. А так как, напомню, я пришёл работать именно новоиспечённым аналитиком, то для решения такого рода проблем мне очень приглянулась идеалогия pipeline'ов, которые используются в pandas для построения алгоритмов обработки данных с последующим обучением моделек. В общем, каждый ETL процесс проекта мне хотелось в видеть в виде последовательности шагов, выполняющихся один за другим. Тогда получаем:\n",
    "\n",
    "* Pipeline -- ETL процесс, состоящий из последовательного выполнения шагов, принимающий на вход таблицы из БД, результатом выполнения которого является новая таблица в БД (можно несколько, но желательно небольшое количество, иначе получается много намешанной логики)\n",
    "* Step -- Шаг вычислений, на вход принимает спарковские датафреймы или обращается к базам напрямую. На выходе одна или несколько таблиц, которые будут использоваться в дальнейших вычислениях \n",
    "\n",
    "### Step\n",
    "Пойдём от малого к большому и начнём с описания требований к шагу вычислений. Начнём с объйма -- шаг не должен быть большим. Обычно, в практике мы ориентировались на 100-150 строк спаркокода. Такое количество приемлемо, чтобы не потерять цепочку повествования даже при весьма заковыристой логике. Помимо этого, в каждом шаге хотелось бы иметь описание структур входных и выходных таблиц и в процессе вычислений сверять данные с этой структурой. Получаем следующие требования к объекту **Step**:\n",
    "\n",
    "* Так как мы целимся на нормальную догументацию, каждый класс-шаг должен иметь качественное описание логики его вычислений\n",
    "* Должен иметь атрибуты, в которых хранятся описания схем входных и выходных таблиц\n",
    "* Должен иметь функцию, в которой, собственно, и будут производиться все основные вычисления\n",
    "* Должен иметь функцию `.run()`, которая будет возвращать список датафреймов, являющихся результатами вычислений\n",
    "\n",
    "Написание кода в такой парадигме будет осуществлятся следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fdea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.module_base import StepBase\n",
    "# Создание класса вычислений\n",
    "class SomeStep(StepBase):\n",
    "    \"\"\"\n",
    "    Название шага\n",
    "    \n",
    "    Описание вычислений класса в формате Markdown\n",
    "    \"\"\"\n",
    "    source_tables = {  # словарь описаний источников\n",
    "        'input_table_1': {  # имя, по которому можно будет обращаться к таблице при описании вычислений\n",
    "            'link': 'sales_tbl',  # ссылка на таблицу (можно алиасом из config_sources, как здесь), \n",
    "                                 # 'argument' если таблица не из БД\n",
    "            'description': 'Таблица для теста',  # описание таблицы\n",
    "            'columns': [  # список колонок\n",
    "                ('zrtpluid', 'string', 'plu', None), # имя, тип колонки в таблице, новое имя, новый тип\n",
    "                ('zprice', 'double', 'price', None),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_tables = {\n",
    "        'out_table_1': {\n",
    "            'link': None,  # ссылка на таблицу (можно алиасом из config_sources), для дальнейшего сохранения\n",
    "            'description': 'Таблица после теста',\n",
    "            'columns': [\n",
    "                ('plu', 'string'), # имя и тип колонки в выхоной таблице таблице\n",
    "                ('price', 'double'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def _calculations(self):\n",
    "        \"Вычисления класса, на выходе словарь в соответствии с self.output_tables\"\n",
    "        df = (\n",
    "            self.tables['input_table_1']\n",
    "            .withColumn('double_price', F.col('price') * 2)\n",
    "            .withColumn('price', F.round('price', 0))\n",
    "        )\n",
    "        return {'out_table_1': df}\n",
    "    \n",
    "# Инициализация объекта класса\n",
    "df = spark.createDataFrame([['555', 13.4]], ['zrtpluid', 'zprice'])\n",
    "argument_tables = {'input_table_1': df}  # Словарь с таблицами-аргументами\n",
    "step = SomeStep(spark, config, argument_tables, test=True, logger=logger)\n",
    "\n",
    "# Запуск вычислений\n",
    "result = step.run()\n",
    "result['out_table_1'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332163b",
   "metadata": {},
   "source": [
    "При этом на стадии инициализации объекта класса осуществляются:\n",
    "* проверки на совпадение названий и типов колонок таблиц на входе с описанием, заданным в `source_tables`\n",
    "* селект, переименование и изменение типов входных таблиц таким образом, чтобы они соответствовали описанию в `source_tables`\n",
    "* складирование таблиц в переменную класа `self.tables`\n",
    "\n",
    "На стадии запуска вычислений осуществается запуск функции `_calculations()` с последующей проверкой на то, что результат её вычислений соотносится с описанием `output_tables`.\n",
    "\n",
    "На первый взгляд, такой подход кажется избыточно сложным. В данном примере написание простейшей операции потребовало значительных затрат. Но когда дело касается более замысловатой логики, он себя более чем оправдывает, делая погружение в такой код много более приятным. Проверки на соответствие типов входных таблиц описанию ни раз спасали, как при дебаге нового кода, так и при рефакторинге старого. Несколько спорным является момент с добавлением в описание нового названия и типа колонок входных таблиц, ведь переименовывание колонок и смена типизации нужна не всегда. Тем не менее, по моему личному ощущению это намного приятнее, чем начинать каждую новую функцию с `.withColumnRenamed` и `cast`.\n",
    "\n",
    "Также хочется отдельно обговорить параметр `tets` при инициализации шага. Как видно в описании класса, ссылка на входную таблицу была задана алиасом из конфига. Если бы мы просто инициализировали шаг, то он бы начал стучаться в базу для того, чтобы получить схему таблицы и сравнить её с описанием. Но для тестирования нам бы хотелось просто передавать шагу на вход нужную нам таблицу. Для этого и вводится параметр `test`, который говорит шагу, что все источники переданы ему в качестве аргументов и в базы ломиться ему не нужно. \n",
    "\n",
    "### Pipeline\n",
    "\n",
    "С модулем Pipeline дела обстоят много проще. Основной принцип его действия:\n",
    "* создаём пустой словарь с таблицами `result`\n",
    "* начинаем последовательно запускать шаги, передавая каждому шагу на вход `result` и обогащая `result` рузультатом выполнения шага\n",
    "* после всех вычислений выбираем те таблицы, которые указаны в описании из `result`\n",
    "\n",
    "Для демонстрации написания кода пайплайна можно написать ещё один шаг, который будет производить те же самые вычисления, но название входной таблицы изменится на `out_table_1`, а выходной на `out_table_2`. Я вынесу за скобки этот копипаст и перейдём к примеру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81895763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.module_base import PipelineBase\n",
    "class Pipeline(PipelineBase):\n",
    "    \"\"\"\n",
    "    Название пайплана\n",
    "    \n",
    "    Общее описание всего пайплайна вычислений\n",
    "    \"\"\"\n",
    "    step_sequence = [\n",
    "        SomeStep,\n",
    "        AnotherStep\n",
    "    ]\n",
    "    \n",
    "    output_tables = {  # Аналогично описанию в Step\n",
    "        'out_table_2': {\n",
    "            'link': None,\n",
    "            'description': 'Финальная таблица',\n",
    "            'columns': [\n",
    "                ('plu', 'string'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Инициализация пайплайна\n",
    "pipeline = Pipeline(spark, config, test_arguments=argument_tables, logger=logger)\n",
    "\n",
    "# Запуск вычислений\n",
    "result = pipeline.run()\n",
    "result['out_table_2'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9800fd",
   "metadata": {},
   "source": [
    "При инициализации пайплацин сначала пробегается последовательно по всем шагам и проверяет возможность вычислений исодя из описаний входных-выходных таблиц каждого шага. Коротко говоря, проверяет, соответствует ли схема выходной таблице шага \"1\" схеме входной таблицы шага \"2\", если шаг \"2\" пользуется результатом вычислений шага \"1\". Для этого не требуется даже инициализация обектов каждого шага. Если со схемами всё в порядке, то начинаются последующие вычисления в соответствии с основынми принципами, которые были представлены в начале раздела.\n",
    "\n",
    "Важным моментом здесь является то, что в логику Pipeline закладывалось то, что он работает только с базами данных и не принимает ничего на вход. Но для теста это не удобно, поэтому нужно было предусмотреть подачу ему на вход <<игрушечных>> таблиц `test_arguments`. Если эти аргументы переданы, то при инициализации Pipeline будет запускать все свои шаги с параметром `test=True` и доступ к базам не потребуется.\n",
    "\n",
    "Описание класса Pipeline следует хранить в файле `module.py` для каждого конкретного модуля. Описание всех шагов может не уложиться в этот файл, поэтому можно растаскать их по другим файлам, но описание именно паплайна лучше хранить для каждого модуля в файле с одинаковым названием. Далее будет показано почему это особенно удобно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb06ed",
   "metadata": {},
   "source": [
    "## Вкусненькое\n",
    "\n",
    "Что же мы получаем, при использовании такой структуры?\n",
    "\n",
    "* Во-первых, использование качественного конфига в виде класса позволяет при любых вычислениях иметь его под рукой (он является обязательным аргументом при инициализации классов типа PipelineBase и передаётся во все его шаги) и делает доступ к параметрам простым и удобным. Особенно удобно, например, описать в конфиге функцию, позволяющую преобразовывать дату вычислений (которая скорее всего будет передаваться любому скрипту как параметр) в указанный формат. В моей практике одна такая функция позволила сокртить код не менее чем на сотню строк по всему проекту.\n",
    "\n",
    "* Во-вторых, при таком подходе сильно сокращается количество строк в файле `run.py`, который является криптом запуска модуля. Обычно, после всех импортов он выглядит как-то так:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    config.parse_arguments()\n",
    "    config.tune_logger(LOGGER)\n",
    "\n",
    "    spark = load_spark()\n",
    "\n",
    "    try:\n",
    "        pipeline = module.Pipeline(spark, config, logger=LOGGER)\n",
    "        result = pipeline.run()\n",
    "        pipeline.save_result_to_hive(save_mode='append')\n",
    "    finally:\n",
    "        spark.stop()\n",
    "```\n",
    "\n",
    "* В-третьих, на своём опыте заметил, что написание кода в концепции StepBase мотивирует людей к написанию функций разумного размера. Функции величиной в пару строк писать не хочется, потому что больше времени потратишь на описание входных-выходных таблиц. Функции величиной в 500-1000 строк тоже нет большого желания писать, так как придётся описывать их логику понятным для неподготовленного читателя языком в описании класса. Таким образом получается что средний пайплайн в нашем проекте состоял из 5-7 шагов с величиной каждого шага 100-200 строк кода. Погружаться в логику работы модулей при таких обстоятельствал стало не только возможно, но и достаточно приятно.\n",
    "\n",
    "* В-четвёртых, как мы уже успели убедиться, тестирование каждого шага является достаточно простым делом. Все таблицы, которые используются для вычислений можно достаточно просто передать на вход и не нужно ломать себе голову над тем, как вынести все инициализации `spark.table()` из тела функции.\n",
    "\n",
    "Но это ещё не всё. Одним из самых приятных моментов, что при таком подходе очень просто собирать описание работы пайплайнов! У нас для этого есть всё:\n",
    "* описание каждого шага на понятном языке (насколько это возможно)\n",
    "* описание входов и выходов каждого шага\n",
    "* краткое описание алгоритма в целом (его цели, время запуска и т.п. лучше как раз и указывать в описании класса Pipeline)\n",
    "\n",
    "Описание, сгенерированное автоматически из объекта класса Pipeline, описанием которого мы занимались на протяжении статьи будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8390e6",
   "metadata": {},
   "source": [
    "---\n",
    "**Название пайплана**\n",
    "\n",
    "\n",
    "Общее описание всего пайплайна вычислений\n",
    "\n",
    "<img src=\"pipeline.png\" width=300>\n",
    "\n",
    "**Модуль состоит из последовательного выполнения следующих шагов:**\n",
    "* **SomeStep** (Название шага):\n",
    "\n",
    "    Описание вычислений класса в формате Markdown\n",
    "\n",
    "  *Исходные таблицы:*\n",
    "\n",
    "\t* input_table_1 (argument) (Таблица для теста)\n",
    "\n",
    "  *Выходные таблицы:*\n",
    "\n",
    "\t* out_table_1 (Таблица после теста)\n",
    "\n",
    "* **AnotherStep** (Шаг, описание которого осталось за скобками):\n",
    "\n",
    "    Делает то же, что и предыдущий:\n",
    "    * удваивает цену, записывает результат в колонку `double_price`\n",
    "\n",
    "  *Исходные таблицы:*\n",
    "\n",
    "\t* out_table_1 (Результат вычислений шага SomeStep) (Таблица после теста)\n",
    "\n",
    "  *Выходные таблицы:*\n",
    "\n",
    "\t* out_table_2 (Таблица после второго теста)\n",
    "\n",
    "\n",
    "**Результатом выполнения алгоритма являются следующие таблицы:**\n",
    "* out_table_2 (Результат вычислений шага AnotherStep) (Финальная таблица):\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "  <tr>\n",
    "    <th>Название колонки</th><th>Формат</th>\n",
    "  </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "  <tr>\n",
    "    <td>plu</td><td>string</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>double_price</td><td>double</td>\n",
    "  </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac440451",
   "metadata": {},
   "source": [
    "Графы вычислений здесь отрисован с помощью утилиты [graphviz](https://graphviz.org). Сам код описания графа достаточно просто сгенерировать из пайплайна, аналогично документации. Печать графа в png можно осуществить при помощи утилиты `dot` на Linux иди Mac. Более того, если класс Pipeline будет приутствовать в файле `module.py` каждого модуля, то достаточно просто будет реализовать функцию, которая будет пробегать по всему проекту и в каждом модуле генерировать его описание в файл README.md. Затем это действие можно делегировать git CI и вообще забыть о том, что описание модулей нужно править где-то помимо кода проекта. Получаем качественное описание алгоритмов (с картинками, чтобы было нескучно читать), на поддержание которой в актуальном состоянии требуется минимум усилий. Профит!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd5efb",
   "metadata": {},
   "source": [
    "## Итого"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c950d5d",
   "metadata": {},
   "source": [
    "В данной статье у меня не было цели подробно разобрать концепцию пайплайнов при написании кода на спарке со всеми тонкостями её реализации. Целью являлось просто привести основные принципы, которые позволяют:\n",
    "\n",
    "* писать код понятным и последовательным образом\n",
    "* сократить время погружения в проект новых сотрудников\n",
    "* писать тесты без боли\n",
    "* составлять понятную лбычным людям документацию и легко поддерживать её в актуальном состоянии\n",
    "\n",
    "Я постарался оставить в статье минимум кода, лишь для демонстрации примеров, чтобы донести основные направления мысли, которыми я руководствовался, реализуя данный функционал. Не претендую на то, что такое решение является единственно верным, но как мне, так и моим коллегам работать в такой парадигме оказалось приятно. Надеюсь, поможет это и кому-нибудь ещё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9c892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8452316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeStep(StepBase):\n",
    "    \"\"\"\n",
    "    Название шага\n",
    "    \n",
    "    Описание вычислений класса в формате Markdown\n",
    "    \"\"\"\n",
    "    source_tables = {  # словарь описаний источников\n",
    "        'input_table_1': {  # имя, по которому можно будет обращаться к таблице при описании вычислений\n",
    "            'link': 'sales_tbl',  # ссылка на таблицу (можно алиасом из config_sources, как здесь), \n",
    "                                 # 'argument' если таблица не из БД\n",
    "            'description': 'Таблица для теста',  # описание таблицы\n",
    "            'columns': [  # список колонок\n",
    "                ('zrtpluid', 'string', 'plu', None), # имя, тип колонки в таблице, новое имя, новый тип\n",
    "                ('zprice', 'double', 'price', None),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_tables = {\n",
    "        'out_table_1': {\n",
    "            'link': None,  # ссылка на таблицу (можно алиасом из config_sources), для дальнейшего сохранения\n",
    "            'description': 'Таблица после теста',\n",
    "            'columns': [\n",
    "                ('plu', 'string'), # имя и тип колонки в выхоной таблице таблице\n",
    "                ('price', 'double'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def _calculations(self):\n",
    "        \"Вычисления класса, на выходе словарь в соответствии с self.output_tables\"\n",
    "        df = (\n",
    "            self.tables['input_table_1']\n",
    "            .withColumn('double_price', F.col('price') * 2)\n",
    "            .withColumn('price', F.round('price', 0))\n",
    "        )\n",
    "        return {'out_table_1': df}\n",
    "class AnotherStep(StepBase):\n",
    "    \"\"\"\n",
    "    Шаг, описание которого осталос за скобками\n",
    "    \n",
    "    Делает то же, что и предыдущий:\n",
    "    * удавивает цену, записывает результат в колонку `double_price`\n",
    "    \"\"\"\n",
    "    source_tables = {  # словарь описаний источников\n",
    "        'out_table_1': {  # имя, по которому можно будет обращаться к таблице при описании вычислений\n",
    "            'link': 'argument',  # ссылка на таблицу (можно алиасом из config_sources), \n",
    "                                 # 'argument' если таблица не из БД\n",
    "            'description': 'Таблица для теста',  # описание таблицы\n",
    "            'columns': [  # список колонок\n",
    "                ('plu', 'string', 'plu', None), # имя, тип колонки в таблице, новое имя, новый тип\n",
    "                ('price', 'double', 'price', None),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_tables = {\n",
    "        'out_table_2': {\n",
    "            'link': None,  # ссылка на таблицу (можно алиасом из config_sources), для дальнейшего сохранения\n",
    "            'description': 'Таблица после теста',\n",
    "            'columns': [\n",
    "                ('plu', 'string'), # имя и тип колонки в выхоной таблице таблице\n",
    "                ('price', 'double'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def _calculations(self):\n",
    "        \"Вычисления класса, на выходе словарь в соответствии с self.output_tables\"\n",
    "        df = (\n",
    "            self.tables['out_table_1']\n",
    "            .withColumn('double_price', F.col('price') * 2)\n",
    "            .withColumn('price', F.round('price', 0))\n",
    "        )\n",
    "        return {'out_table_2': df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483e650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
