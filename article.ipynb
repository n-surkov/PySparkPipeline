{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c77cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('./example_project')\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.master('local[1]').appName('my_app').getOrCreate()\n",
    "import logging\n",
    "logger = logging.getLogger('jupyter_logger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdbe68",
   "metadata": {},
   "source": [
    "# Построение архитектуры проекта при работе с PySpark\n",
    "\n",
    "В настоящее время уже сложно найти достаточно крупную компанию, которая не использовала бы возможности накопления и использования больших данных. Одним из популярных инструментов для построения процессов обработки этих данных является pyspark. В данной статье будет представлен один из методов написания кода на pyspark таким образом, чтобы он был более читаем, легко тестируем и поддерживаем. Сразу оговорюсь, что не представляю это, как единственное правильное решение, но оно доказало свою жизнеспособность на примере того проекта, в котором я работал.\n",
    "\n",
    "Перед тем, как перейти к конкретике, хочу немного рассказать про историю проекта, который натолкнул меня на мысли о проработке архитектуры написания кода. Большинство проектов начинаются с идеи, разрастающейся до MVP в достаточно короткие сроки. На стадии MVP основная задача продукта – показать то, что он работает и может приносить деньги. Если он этого не может, он дальше не живёт, а если может, то из него уже хотят получить готовое решение с минимальными затратами и максимальными темпами. Основная проблема тут заключается в том, что при создании MVP в код продукта вряд ли закладывались все заделы на будущее масштабирование и стабильную работу. Плюс к тому в течении эксплуатации возникает ряд вопросов и доработок для того, чтобы оно вообще работало на постоянку, а не в какой-то ограниченный промежуток времени. В общем, если даже в MVP изначально закладывалась какая-то архитектура, то он скорее не выдержит наплыва доработок и код превращается в что-то вроде наушников, которые пролежали в кармане на протяжении последнего месяца. Вроде ты и пытался сложить их аккуратненько, но доставая их концы найти весьма затруднительно. Проблема усугубляется тем, что люди, которые этот код писали, ещё более-менее в нём ориентируются, но вот для вновь прибывших сотрудников (а это не редкость при расширении проекта) погружение в такой код занимает неоправданно много времени.\n",
    "\n",
    "Вот приблизительно на первой стадии раскатки MVP я и подключился в проект со всей его витиеватой логикой. Здесь я пропущу рассказ о моей боли, которую я испытывал при попытках составить последовательность логических операций по формированию одной таблички, которую я кропотливо собирал по 7 вкладкам кода. Скажу лишь, что именно это и подтолкнуло меня к созданию такой архитектуры, которая позволила облегчить жизнь тем, кто будет после. Надеюсь, облегчит кому-нибудь ещё. Вперёд к светлому будущему!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205879e",
   "metadata": {},
   "source": [
    "## Что не так?\n",
    "\n",
    "Прежде чем решать проблему, нужно её определить. Поэтому для начала я хотел бы обозначить основные моменты, которые были самыми критичными на мой взгляд.\n",
    "\n",
    "* **Документация.** Здесь речь идёт даже не о банальных правилах расставления комментариев и описаний функций. Они, конечно, важны, намого проще разобраться в коде, если там есть хоть какие-то пояснения, но есть и другая проблема. Наш продукт, например, находится в постоянном контакте с бизнесом. Логика его работы должна быть этому бизнесу понятна и где-то зафиксирована. При этом, естественно, в источнике, доступном бизнесу, она должна поддерживаться в актуальном состоянии. Даже если аккуратно описать все функции на понятном человеку языке, сгенерировать какой-нибудь [pydoc](https://docs.python.org/3/library/pydoc.html) или [sphinx](https://www.sphinx-doc.org/en/master/), то вряд ли вы порадуете менеджеров, если дадите им это почитать. Есть другой путь – написать менеджерам отдельное описание логики проекта. Но в таком случае при любом изменении логики (а они происходили постоянно), придётся поменять описание внутри кода, поменять описание функции, а затем ещё и переписать документацию для бизнеса. Вывод – хочется иметь такую документацию, которая была бы понятна и людям, работающим с кодом и бизнесу. А ещё неплохо бы тратить на неё не больше времени, чем на написание кода.\n",
    "\n",
    "* **Нелинейность кода.** С одной стороны код типа ```fun(fun1(fun2…))``` очень плохо воспринимается, а ситуации, когда он прямо таки необходим в ETL процесса встречаются крайне редко. С другой стороны, возвращаемся к вопросу документации и объяснения этой логики. Чем больше вложенности, тем сложнее восстановить логику в один связный текст.\n",
    "\n",
    "* **Недекомпозированность кода.** Работа датафреймами в pyspark ни в коем разе не останавливает полёт фантазии. Если у вас достаточно запала то написание функции можно не останавливать никогда. ```df = select.join.withColumn.withColumn.groupby.agg.select.withColumnRenamed...``` и дальше, дальше пока не оставят силы. Можно отдохнуть. А на следующий день ```df_new = df.select.join.withColumn...```. В общем, если без шуток, то я натыкался на файлы в несколько тысяч строк вот такого кода, в котором уже на 500 строчке забываешь, с чего, собственно, всё начиналось. Ещё одной большой проблемой длинного спарковского кода лично для меня является то, что очень быстро перестаёшь осознавать, какие вообще колонки есть в том или ином датафрейме.\n",
    "\n",
    "* **Сложность тестирования.** В основном, функции содержали обращения к таблицам базы внутри себя. В таких условиях очень тяжело их хоть как-то тестировать, так как тесты на больших таблицах занимают очень много времени, а при отсутствии доступа к базе вообще невозможны, а хотелось бы. Можно, конечно, собрать свою локальную песочницу и записать все тестовые таблицы туда. Но тесты частенько помогают понять логику кода, и хотелось бы иметь тестовые таблички всегда под рукой.\n",
    "\n",
    "Теперь, когда основные проблемы обозначены, можем перейти к примерам их решения. Далее я не хотел бы уделять время скучному разбору кода базовых классов, на которых строилось моё решение. Больше внимания я уделю основным концепциям с примерчиками использования, а сам код вы всегда сможете найти в [репозитории](https://github.com/n-surkov/PySparkPipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2071451",
   "metadata": {},
   "source": [
    "## Структура проекта\n",
    "\n",
    "Проекты разные, задачи разные, структуры разные. Говорить, что какая-то структура является универсальной смысла не имеет. В этом разделе просто будет приведён пример структуры, которая хорошо прижилась в нашем проекте, и в контексте которой я буду дальше вести повествование. Возьмём только минимально необходимое:\n",
    "\n",
    "```\n",
    "example_project:\n",
    "|--utils\n",
    "|  |--description_builder.py\n",
    "|--modules\n",
    "|  |--__init__.py\n",
    "|  |--module_base.py\n",
    "|  |--config_base.py\n",
    "|  |--module1\n",
    "|     |--__init__.py\n",
    "|     |--config.py\n",
    "|     |--module.py\n",
    "|     |--runner.py\n",
    "|--config.yml\n",
    "|--config_sources.yml\n",
    "```\n",
    "\n",
    "На самом верхнем уровне у нас лежат конфиги проекта. У нас было всего 2 файла, поэтому в корне они никому не мешались (`config.yml`, `config_sources.yml`). Если их больше, то, конечно, можно уместить их в отдельную директорию. Дальше директория со скриптами-утилитами (`utils`) и, собственно, дерево всех модулей проекта (`modules`).\n",
    "\n",
    "Модули проекта -- его логические части. Здесь изображена структура модулей одного уровня вложенности~~, потому что я устал рисовать~~ для простоты восприятия, но уровней может быть и больше. Главное, чтобы они были логически связаны. В каждом модуле лежит файл с конфигом этого модуля (`config.py`), файл в котором содержатся все вычисления (`module.py`) и скрипт для запуска (`runner.py`), в котором должно быть минимум кода. Файлы `module_base.py` и `config_base.py` содержат базовые классы, речь о которых пойдёт далее. Да, хранить базовые классы именно так не самое удачное решение, но оно работает, никого не беспокоит, поэтому пусть полежат здесь.\n",
    "\n",
    "Пример с такой структурой можно наблюдать в разделе [репозитория](https://github.com/n-surkov/PySparkPipeline/tree/main/example_project).\n",
    "\n",
    "Теперь, когда мы разобрались с основой структуры, можем переходить, собственно, к самому решению, которое, на мой взгляд, сделало жизнь в проекте намного проще."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a932f8e",
   "metadata": {},
   "source": [
    "## Конфиги проекта\n",
    "\n",
    "Начнём с конфигураций проекта. В большинстве случаев конфиги делятся на два типа:\n",
    "* **статические**, которые иногда меняются, но всё-таки крайне редко\n",
    "* **динамические**, которые может понадобиться менять чуть ли ни при каждом отдельном запуске скрипта. Обычно они же и передаются в качестве параметров в скриптах запуска\n",
    "\n",
    "Начнём со статических конфигов. Обычно это словарь, полученный при чтении yaml, json или подобного файла, где задана конфигурация проекта. В нашем случае это два yaml-файла. `config.yml` -- файл, содержащий параметры, общие для всего проекта. В  нашем проекте это были сопоставления городов и макрорегионов. `config_sources.yml` -- файл, содержащий список витрин, которые используются в проекте. Источники могут быть внешними (считаются другими продуктами) и внутренними (считаются внутри проекта). Также, под проект может быть выделено несколько баз (для прода, теста, бэкапов и т.д.) и всю эту информацию нужно как-то хранить. В своём проекте мы пришли к следующей схеме описания таблиц:\n",
    "\n",
    "```yaml\n",
    "# Внешние источники\n",
    "goods_tbl: 'edw_db.goods_table' # справочник товаров (ссылка)\n",
    "sales_tbl: 'edw_db.sales_table' # продажи магазинов (ссылка)\n",
    "\n",
    "# Базы данных продукта\n",
    "db_backups:\n",
    "    test: 'product_test_db' # база, в куторую сохраняются бэкапы расчётов\n",
    "    prod: 'product_prod_db' # база, в куторую сохраняются расчёты\n",
    "\n",
    "# Бэкапы\n",
    "backups: # витрины продукта, слепок которых делается в тестовую базу\n",
    "    product_table_1: # алиас таблицы\n",
    "        table_name: table_name_in_database # имя таблицы в базе\n",
    "        partitionedby: ['date', 'city'] # партиции\n",
    "```\n",
    "\n",
    "Сначала идёт перечисление внешних источников, которые мы только считываем. Затем список баз данных продукта, в нашем случае это продовая и тестовая базы. Затем идёт описание таблиц, расчёт которых производится в проекте. Здесь полезной дополнительной информацией будет список колонок, по которым производится партиционирование таблицы при записи.\n",
    "\n",
    "Если говорить о динамических конфигах, то у нас они всегда передавались через bash в качестве аргументов. В силу этого, каждый скрипт запуска начинается с добавления параметров в `argparse`, причём обычно, всё это полотно задаётся кописастом из загрузчика ранее написанного модуля. В силу этого, такие параметры тоже хотелось бы хранить в нашем конфиге.\n",
    "\n",
    "Ну и наконец, во время работы скриптов может понадобиться передавать некоторую информацию из функции в функцию. Конфиг, скорее всего будет тем объектом, который передаётся в каждую функцию и передавать параметры через него достаточно удобно.\n",
    "\n",
    "Итого, конфиг в проекте будет объектом класса, в котором существуют 4 атрибута:\n",
    "* `cfg` -- для статических конфигов из `config.yml` (считывается при инициализации объекта класса)\n",
    "* `cfg_sources` -- для хранения информации по базам данных из `config_sources.yml` (считывается при инициализации объекта класса)\n",
    "* `parameters` -- параметры, получающие данные при помощи `argparse`. Описание параметров, которые используются повсеместно (таких как, например, TEST -- флаг использования тестовой базы, или дата расчёта), можно задавать в `__init__` базового класса\n",
    "* `tmp` -- дополнительные данные, которые могут быть добавлены в конфиг в процессе вычислений\n",
    "\n",
    "А также несколько полезных функций:\n",
    "* `getitem`, `setitem` -- для того, чтобы к классу можно было обращаться, как к словарю\n",
    "* `print_description` -- для выведения описания всех конфигураций\n",
    "* `add_parameter`, `parse_arguments` -- для добавления параметров и парсинга их значений из bash\n",
    "* `get_table_link` -- функция, выдающая полный путь к таблице по её алиасу \n",
    "\n",
    "В конфиг можно также накидать и других функций, результат которых зависит от статических или динамических параметров, и которые используются повсеместно в проекте. Например, функцию, которая возвращает дату расчёта в нужном формате очень упрощает жизнь.\n",
    "\n",
    "Ниже пример использования конфига для получения ссылки на таблицу `product_table_1` из описания выше. В конфиге по умолчанию, добавлен параметр `TEST`, который определяет базу данных проекта для проведения вычислений. Также продемонстрировано, как работать с параметрами и временными значениями конфига."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2344cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test parameter 'False' table link is 'product_prod_db.table_name_in_database'\n",
      "For test parameter 'True' table link is 'product_test_db.table_name_in_database'\n",
      "---------------------------------------------------\n",
      "global_config contains:\n",
      "\n",
      "config_parameter_1 : 12\n",
      "config_parameter_2 : \n",
      "\tsubparam1 : param\n",
      "\tsubparam2 : noparam\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "sources_config contains:\n",
      "\n",
      "goods_tbl : edw_db.goods_table\n",
      "sales_tbl : edw_db.sales_table\n",
      "db_backups : \n",
      "\ttest : product_test_db\n",
      "\tprod : product_prod_db\n",
      "backups : \n",
      "\tproduct_table_1 : \n",
      "\t\ttable_name : table_name_in_database\n",
      "\t\tpartitionedby : ['date', 'city']\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "parameters contain:\n",
      "\n",
      "TEST : True\n",
      "calc_date : 21.08.2021\n",
      "logging_level : 10\n",
      "validateAS : 0\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "temporary data contains:\n",
      "\n",
      "tmp_parameter : None\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from modules.config_base import ConfigBase\n",
    "# Пример вывода ссылки на таблицу\n",
    "config = ConfigBase()\n",
    "print(f\"For test parameter '{config['TEST']}' table link is '{config.get_table_link('product_table_1')}'\")\n",
    "config.update_config({'TEST': True})\n",
    "print(f\"For test parameter '{config['TEST']}' table link is '{config.get_table_link('product_table_1')}'\")\n",
    "\n",
    "# Добавление параметров\n",
    "config.add_parameter(\"validateAS\", '--vas', type=int, required=False,\n",
    "                     default=0,\n",
    "                     help=\"\"\"Валидировать ли вычисления шага AnotherStep\"\"\"\n",
    "                     )\n",
    "config['tmp_parameter'] = None\n",
    "config.print_description()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4625b40",
   "metadata": {},
   "source": [
    "Даже такая простая функция, как возвращение пути к таблице по её алиасу, может существенно сократить количество однотипного кода в проекте. На функцию печати содержания конфига тоже рекомендую потратить время, это экономит много времени в дальнейшем, когда модулей в проекте становится больше, чем один.\n",
    "\n",
    "В заключении следует оговорить, что же тогда содержится в файлах `config.py` каждого отдельного модуля? В этих файлах происходит инициализация объекта базового класса `ConfigBase` и добавление в этот объект параметров, имеющих отношение к модулю (функция `add_parameter()`).\n",
    "\n",
    "**Нужно больше кода?**\n",
    "\n",
    "* Шаблон базового класса находится в [библиотеке репозитория](https://github.com/n-surkov/PySparkPipeline/blob/main/sparkpip/config_base.py)\n",
    "* Пример адаптации конфига под конкретный проект находится в файле [config_base.py](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/modules/config_base.py) проекта [example_project](https://github.com/n-surkov/PySparkPipeline/tree/main/example_project)\n",
    "* Пример дополнения конфига для конкретного модуля также лежит в [соответствующем файле](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/modules/some_module/config.py)\n",
    "* Файлы [config.yml](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/config.yml) и [config_sources.yml](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/config_sources.yml), о которых шла речь, также можно найти в примере репозитория."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb1b11",
   "metadata": {},
   "source": [
    "## Линеаризация структуры\n",
    "\n",
    "Иметь хороший конфиг, конечно, приятно, но [глобальных проблем](#Каким-же-должен-быть-код?) это не решает. Основной из них, на мой взгляд, является нелинейность написания кода. Для решения такого рода проблемы мне очень приглянулась идеология pipeline'ов, которые используются в pandas для построения алгоритмов обработки данных с последующим обучением моделек. В общем, каждый ETL процесс проекта мне хотелось видеть в виде последовательности шагов, выполняющихся один за другим. В моей терминологии я определил следующие два основных понятия:\n",
    "\n",
    "* Pipeline -- ETL процесс, состоящий из последовательного выполнения шагов, принимающий на вход таблицы из БД, результатом выполнения которого является новая таблица в БД (можно несколько, но желательно небольшое количество, иначе получается много намешанной логики)\n",
    "* Step -- Шаг вычислений, на вход принимает `spark.DataFrame` или обращается к базам напрямую. На выходе одна или несколько таблиц (в виде тех же `spark.DataFrame`), которые будут использоваться в дальнейших вычислениях \n",
    "\n",
    "Под каждый из этих пунктов был написан свой базовый класс, основные концепции которых я и попытаюсь продемонстрировать далее.\n",
    "\n",
    "### Step\n",
    "\n",
    "Пойдём от малого к большому и начнём с описания требований к шагу вычислений. Начнём с объёма -- шаг не должен быть большим. Обычно, в практике мы ориентировались на 100-150 строк спаркокода. Такое количество приемлемо, чтобы не потерять цепочку повествования даже при весьма заковыристой логике. Помимо этого, в каждом шаге хотелось бы иметь описание структур входных и выходных таблиц и в процессе вычислений сверять данные с этой структурой. Получаем следующие требования к объекту **Step**:\n",
    "\n",
    "* Так как мы целимся на нормальную документацию, каждый класс-шаг должен иметь качественное описание логики его вычислений\n",
    "* Должен иметь атрибуты, в которых хранятся описания схем входных и выходных таблиц\n",
    "* Если у нас есть описания таблиц, то пусть он проверяет вход на соответствие описанию перед началом вычислений \n",
    "* Должен иметь функцию, в которой, собственно, и будут производиться все основные вычисления\n",
    "* Должен иметь функцию `.run()`, которая будет возвращать список датафреймов, являющихся результатами вычислений. Отдельная функция тут нужна для того, чтобы проверять на соответствие описанию и выходные таблицы\n",
    "\n",
    "Написание кода в такой парадигме будет осуществляться следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7fdea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------+\n",
      "|plu|price|double_price|\n",
      "+---+-----+------------+\n",
      "|555| 13.0|        26.8|\n",
      "+---+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Импортируем базовый класс шага\n",
    "from modules.module_base import StepBase\n",
    "# Создание класса вычислений\n",
    "class SomeStep(StepBase):\n",
    "    \"\"\"\n",
    "    Название шага\n",
    "    \n",
    "    Описание вычислений класса в формате Markdown\n",
    "    \"\"\"\n",
    "    source_tables = {  # словарь описаний источников\n",
    "        'input_table_1': {  # имя, по которому можно будет обращаться к таблице при описании вычислений\n",
    "            'link': 'sales_tbl',  # ссылка на таблицу (можно алиасом из config_sources, как здесь), \n",
    "                                 # 'argument' если таблица не из БД\n",
    "            'description': 'Таблица для теста',  # описание таблицы\n",
    "            'columns': [  # список колонок\n",
    "                ('zrtpluid', 'string', 'plu', None), # имя, тип колонки в таблице, новое имя, новый тип\n",
    "                ('zprice', 'double', 'price', None),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_tables = {\n",
    "        'out_table_1': {\n",
    "            'link': None,  # ссылка на таблицу (можно алиасом из config_sources), для дальнейшего сохранения\n",
    "            'description': 'Таблица после теста',\n",
    "            'columns': [\n",
    "                ('plu', 'string'), # имя и тип колонки в выхоной таблице таблице\n",
    "                ('price', 'double'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def _calculations(self):\n",
    "        \"Вычисления класса, на выходе словарь в соответствии с self.output_tables\"\n",
    "        df = (\n",
    "            self.tables['input_table_1']\n",
    "            .withColumn('double_price', F.col('price') * 2)\n",
    "            .withColumn('price', F.round('price', 0))\n",
    "        )\n",
    "        return {'out_table_1': df}\n",
    "    \n",
    "# Инициализация объекта класса\n",
    "df = spark.createDataFrame([['555', 13.4]], ['zrtpluid', 'zprice'])\n",
    "argument_tables = {'input_table_1': df}  # Словарь с таблицами-аргументами\n",
    "step = SomeStep(spark, config, argument_tables, test=True, logger=logger)\n",
    "\n",
    "# Запуск вычислений\n",
    "result = step.run()\n",
    "result['out_table_1'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332163b",
   "metadata": {},
   "source": [
    "При этом на стадии инициализации объекта класса осуществляются:\n",
    "* проверки на соответствие названий и типов колонок входных таблиц описанию, заданному в `source_tables`\n",
    "* селект, переименование и изменение типов колонок входных таблиц таким образом, чтобы они соответствовали новым названиям и типам `source_tables`\n",
    "* складирование таблиц в переменную класса `self.tables`\n",
    "\n",
    "На стадии запуска вычислений осуществляется выполнение функции `_calculations()` с последующей проверкой на то, что результат её вычислений соотносится с описанием `output_tables`.\n",
    "\n",
    "На первый взгляд, такой подход кажется избыточно сложным. В данном примере написание простейшей операции потребовало значительных затрат. Но когда дело касается более замысловатой логики, он себя более чем оправдывает:\n",
    "* делает погружение в такой код много более приятным \n",
    "* проверки на соответствие типов входных таблиц описанию ни раз спасали, как при дебаге нового кода, так и при рефакторинге старого \n",
    "* сильно сокращает количество строк кода с `.withColumnRenamed` и `cast`. \n",
    "\n",
    "Последний пунт можно назвать несколько спорным, ведь переименовывание колонок и смена типизации нужны не всегда, а писать по 4 параметра в описании каждой колонки придётся везде. Тем не менее, начинать каждую новую функцию с `.withColumnRenamed` и `cast` мне нравится ещё меньше.\n",
    "\n",
    "Также хочется отдельно обговорить параметр `test` при инициализации шага. Как видно в описании класса, ссылка на входную таблицу была задана алиасом из конфига. В этом случае, если бы мы просто инициализировали шаг, то он бы начал стучаться в базу для того, чтобы получить схему таблицы и сравнить её с описанием. Но для тестирования нам бы хотелось просто передавать шагу на вход нужную нам таблицу. Для этого и вводится параметр `test`, который говорит шагу, что все источники переданы ему в качестве аргументов и в базы ломиться ему не нужно.\n",
    "\n",
    "Более подробно пример кода базового класса можно найти в файле [step_base.py](https://github.com/n-surkov/PySparkPipeline/blob/main/sparkpip/step_base.py#L191) библиотеки. В целом, хотелось бы отметить, что основную часть кода составляет логика проверок на соответствие таблиц описанию. Если проверки не осуществлять, то код можно уложить в пару десятков строк.\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "С модулем Pipeline дела обстоят много проще. Основной принцип его действия:\n",
    "* создаём пустой словарь с таблицами `result`\n",
    "* начинаем последовательно запускать шаги, передавая каждому шагу на вход `result` и обогащая `result` результатом выполнения текущего шага\n",
    "* после всех вычислений выбираем из `result` те таблицы, которые указаны в описании\n",
    "\n",
    "Для демонстрации написания кода пайплайна можно написать ещё один шаг, который будет производить те же самые вычисления, что и `SomeStep`, но название входной таблицы изменится на `out_table_1`, а выходной на `out_table_2`. Я вынесу за скобки [этот копипаст](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/modules/some_module/module.py#L46) и перейдём к примеру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81895763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|plu|double_price|\n",
      "+---+------------+\n",
      "|555|        26.0|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Импортируем тот самый шаг и класс базового пайплайна\n",
    "from modules.some_module.module import AnotherStep\n",
    "from modules.module_base import PipelineBase\n",
    "\n",
    "# Определяем пайплайн\n",
    "class Pipeline(PipelineBase):\n",
    "    \"\"\"\n",
    "    Название пайплана\n",
    "    \n",
    "    Общее описание всего пайплайна вычислений\n",
    "    \"\"\"\n",
    "    step_sequence = [\n",
    "        SomeStep,\n",
    "        AnotherStep\n",
    "    ]\n",
    "    \n",
    "    output_tables = {  # Аналогично описанию в Step\n",
    "        'out_table_2': {\n",
    "            'link': None,\n",
    "            'description': 'Финальная таблица',\n",
    "            'columns': [\n",
    "                ('plu', 'string'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Инициализация пайплайна\n",
    "pipeline = Pipeline(spark, config, test_arguments=argument_tables, logger=logger)\n",
    "\n",
    "# Запуск вычислений\n",
    "result = pipeline.run()\n",
    "result['out_table_2'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9800fd",
   "metadata": {},
   "source": [
    "При инициализации пайплайн сначала пробегается последовательно по всем шагам и проверяет возможность вычислений исходя из описаний входных-выходных таблиц каждого шага. Коротко говоря, проверяет, соответствует ли схема выходной таблицы шага \"1\" схеме входной таблицы шага \"2\", если шаг \"2\" пользуется результатом вычислений шага \"1\". Для этого не требуется даже инициализация объектов каждого шага. Если со схемами всё в порядке, то начинаются последующие вычисления в соответствии с основными принципами, которые были представлены в начале раздела.\n",
    "\n",
    "Важным моментом здесь является то, что в логику Pipeline закладывалось то, что он работает только с базами данных и не принимает ничего на вход. Но для теста это не удобно, поэтому нужно было предусмотреть подачу ему на вход <<игрушечных>> таблиц `test_arguments`. Если эти аргументы переданы, то при инициализации Pipeline будет запускать все свои шаги с параметром `test=True` и доступ к базам не потребуется.\n",
    "\n",
    "Описание класса Pipeline следует хранить в файле `module.py` для каждого конкретного модуля. Описание всех шагов может не уложиться в этот файл, поэтому можно растаскать их по другим файлам, но описание именно пайплайна лучше хранить для каждого модуля в файле с одинаковым названием. Далее будет показано почему это особенно удобно.\n",
    "\n",
    "**Нужно больше кода?**\n",
    "* Шаблон для описания работы пайплайна также можно найти [в библиотеке](https://github.com/n-surkov/PySparkPipeline/blob/main/sparkpip/pipeline_base.py#L41). В шаблоне пайплайна, так же как и в шаблоне шага, большую часть кода занимают вспомогательные функции проверок. Также в шаблон добавлены функции записи в базу, которые позволили сократить ещё тонну кода в проекте.\n",
    "* Подтягивание базовых классов в проект происходит в файле [module_base.py](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/modules/module_base.py). В этом файле можно изменить или добавить каких-либо функций относительно шаблона, которые будут полезны именно вам. В примере не происходит ничего, кроме перетягивания шаблонов в проект."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb06ed",
   "metadata": {},
   "source": [
    "## Вкусненькое\n",
    "\n",
    "Что же мы получаем, при использовании такой структуры?\n",
    "\n",
    "* Во-первых, использование качественного конфига позволяет иметь все необходимые параметры под рукой, так как он является обязательным аргументом при инициализации классов типа PipelineBase и передаётся во все его шаги. \n",
    "\n",
    "* Во-вторых, при таком подходе сильно сокращается количество строк в файле `run.py`, который является криптом запуска модуля. Обычно, после всех импортов он выглядит как-то так (Пример раннера, запускающего описанный ранее пайплайн можно посмотреть в [репозитории](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/modules/some_module/runner.py)):\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    config.parse_arguments()\n",
    "    config.tune_logger(LOGGER)\n",
    "\n",
    "    spark = load_spark()\n",
    "\n",
    "    try:\n",
    "        pipeline = module.Pipeline(spark, config, logger=LOGGER)\n",
    "        result = pipeline.run()\n",
    "        pipeline.save_result_to_hive(save_mode='append')\n",
    "    finally:\n",
    "        spark.stop()\n",
    "```\n",
    "\n",
    "* В-третьих, на своём опыте заметил, что написание кода в концепции StepBase мотивирует людей к написанию функций разумного размера. Функции величиной в пару строк писать не хочется, потому что больше времени потратишь на описание входных-выходных таблиц. Функции величиной в 500-1000 строк тоже нет большого желания писать, так как придётся описывать их логику понятным для неподготовленного читателя языком в описании класса. Таким образом получается, что средний пайплайн в нашем проекте состоял из 5-7 шагов с величиной каждого шага 100-200 строк кода. Погружаться в логику работы модулей при таких обстоятельствах стало не только возможно, но и достаточно приятно.\n",
    "\n",
    "* В-четвёртых, как мы уже успели убедиться, тестирование каждого шага является достаточно простым делом. Все таблицы, которые используются для вычислений достаточно просто передать на вход и не нужно ломать себе голову над тем, как вынести все инициализации `spark.table()` из тела функции. Ещё больше примеров тестирования основных шаблонов можно найти [здесь](https://github.com/n-surkov/PySparkPipeline/blob/main/tests/test_module_base.py).\n",
    "\n",
    "Но это ещё не всё. Одним из самых приятных моментов, что при таком подходе очень просто собирать описание работы пайплайнов! У нас для этого есть всё:\n",
    "* описание каждого шага на понятном языке (насколько это возможно)\n",
    "* описание входов и выходов каждого шага\n",
    "* краткое описание алгоритма в целом (его цели, время запуска и т.п. лучше как раз и указывать в описании класса Pipeline)\n",
    "* описание выходных таблиц пайплайна\n",
    "\n",
    "Код генерации описания содержится в функции [get_pipeline_description](https://github.com/n-surkov/PySparkPipeline/blob/main/sparkpip/pipeline_base.py#L299) базового класса пайплайна. Также можно собрать и визуализировать граф вычислений средствами [graphviz](https://graphviz.org) (функция [get_pipeline_graph](https://github.com/n-surkov/PySparkPipeline/blob/main/sparkpip/pipeline_base.py#L398)).\n",
    "\n",
    "Для разобранного нами примера [описание](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/modules/some_module/README.md) будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8390e6",
   "metadata": {},
   "source": [
    "---\n",
    "**Название пайплана**\n",
    "\n",
    "\n",
    "Общее описание всего пайплайна вычислений\n",
    "\n",
    "<img src=\"pipeline.png\" width=300>\n",
    "\n",
    "**Модуль состоит из последовательного выполнения следующих шагов:**\n",
    "* **SomeStep** (Название шага):\n",
    "\n",
    "    Описание вычислений класса в формате Markdown\n",
    "\n",
    "  *Исходные таблицы:*\n",
    "\n",
    "\t* input_table_1 (argument) (Таблица для теста)\n",
    "\n",
    "  *Выходные таблицы:*\n",
    "\n",
    "\t* out_table_1 (Таблица после теста)\n",
    "\n",
    "* **AnotherStep** (Шаг, описание которого осталось за скобками):\n",
    "\n",
    "    Делает то же, что и предыдущий:\n",
    "    * удваивает цену, записывает результат в колонку `double_price`\n",
    "\n",
    "  *Исходные таблицы:*\n",
    "\n",
    "\t* out_table_1 (Результат вычислений шага SomeStep) (Таблица после теста)\n",
    "\n",
    "  *Выходные таблицы:*\n",
    "\n",
    "\t* out_table_2 (Таблица после второго теста)\n",
    "\n",
    "\n",
    "**Результатом выполнения алгоритма являются следующие таблицы:**\n",
    "* out_table_2 (Результат вычислений шага AnotherStep) (Финальная таблица):\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "  <tr>\n",
    "    <th>Название колонки</th><th>Формат</th>\n",
    "  </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "  <tr>\n",
    "    <td>plu</td><td>string</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>double_price</td><td>double</td>\n",
    "  </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac440451",
   "metadata": {},
   "source": [
    "Более того, если придерживаться структуры в проекте, то не составит большого труда написать функцию, которая будет пробегаться по всем модулям проекта, строить для них документацию с картинкой графа вычислений и записывать её в README.md модуля. В корневой README она же может вставлять оглавления со ссылками на частные ридмишки модулей. Пример такой функции можно посмотреть [здесь](https://github.com/n-surkov/PySparkPipeline/blob/main/example_project/utils/description_builder.py). \n",
    "\n",
    "Затем, запуск такой функции можно делегировать git CI и вообще забыть о том, что описание модулей нужно править где-то помимо кода проекта. Получаем качественное описание алгоритмов (с картинками, чтобы было нескучно читать), на поддержание которой в актуальном состоянии требуется минимум усилий. Профит!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd5efb",
   "metadata": {},
   "source": [
    "## Итого"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c950d5d",
   "metadata": {},
   "source": [
    "В данной статье у меня не было цели подробно разобрать концепцию пайплайнов при написании кода на спарке со всеми тонкостями её реализации. Код всегда можно глянуть в [репозитории](https://github.com/n-surkov/PySparkPipeline), ссылки на отдельные куски кода, которые позволяли реализовать тот или иной функционал также своевременно поставлялись. Целью являлось просто привести основные принципы, которые позволяют:\n",
    "\n",
    "* писать код понятным и последовательным образом\n",
    "* сократить время погружения в проект новых сотрудников\n",
    "* писать тесты без боли\n",
    "* составлять понятную обычным людям документацию и легко поддерживать её в актуальном состоянии\n",
    "\n",
    "Не претендую на то, что такое решение является единственно верным, но как мне, так и моим коллегам работать в такой парадигме оказалось приятно. Надеюсь, поможет это и кому-нибудь ещё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be51bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
