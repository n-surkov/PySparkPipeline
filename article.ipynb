{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c77cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./my_project')\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.master('local[1]').appName('my_app').getOrCreate()\n",
    "import logging\n",
    "logger = logging.getLogger('jupyter_logger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdbe68",
   "metadata": {},
   "source": [
    "# Построение архитектуры проекта при работе с PySpark\n",
    "\n",
    "В настоящее время уже сложно найти достаточно крупную компанию, которая не использовала бы возможности накопления и использования больших данных. Одним из популярных инструментов для построения процессов обработки этих данных является pyspark. В данной статье будет представлен один из методов написания кода на pyspark таким образом, чтобы он был более читаем, легко тестируем и поддерживаем. Сразу оговорюсь, что не представляю это, как единственное правильное решение, но оно доказало свою жизнеспособность на примере того проекта, в котором я работал.\n",
    "\n",
    "Перед тем, как перейти к конкретике, хочу немного рассказать про историю проекта, который натолкнул меня на мысли о проработке архитектуры написания кода. Большинство проектов начинаются с идеи, разрастающейся до MVP в достаточно короткие сроки. На стадии MVP основная задача продукта – показать то, что он работает и может приносить деньги. Если он этого не может, он дальше не живёт, а если может, то из него уже хотят получить готовое решение с минимальными затратами и максимальными темпами. Основная проблема тут заключается в том, что при создании MVP в код продукта вряд ли закладывались все заделы на будущее масштабирование и стабильную работу. Плюс к тому в течении эксплуатации возникает ряд вопросов и доработок для того, чтобы оно вообще работало на постоянку, а не в какой-то ограниченный промежуток времени. В общем, если даже в MVP изначально закладывалась какая-то архитектура, то он скорее не выдержит наплыва доработок и код превращается в что-то вроде наушников, которые пролежали в кармане на протяжении последнего месяца. Вроде ты и пытался сложить их аккуратненько, но доставая их концы найти весьма затруднительно. Проблема усугубляется тем, что люди, которые этот код писали, ещё более-менее в нём ориентируются, но вот для вновь прибывших сотрудников (а это не редкость при расширении проекта) погружение в такой код занимает неоправданно много времени.\n",
    "\n",
    "Вот приблизительно на первой стадии раскатки MVP я и подключился в проект со всей его витиеватой логикой. Здесь я пропущу рассказ о моей боли, которую я испытывал при попытках составить последовательность логических операций по формированию одной таблички, которую я кропотливо собирал по 7 вкладкам кода. Скажу лишь, что именно это и подтолкнуло меня к созданию такой архитектуры, которая позволила облегчить жизнь тем, кто будет после. Надеюсь, облегчит кому-нибудь ещё. Вперёд к светлому будущему!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205879e",
   "metadata": {},
   "source": [
    "## Что не так?\n",
    "\n",
    "Прежде чем решать проблему, нужно её определить. Поэтому для начала я хотел бы обозначить основные моменты, которые были самыми критичными на мой взгляд.\n",
    "\n",
    "* **Документация.** Здесь речь идёт даже не о банальных правилах расставления комментариев и описаний функций. Они, конечно, важны, намого проще разобраться в коде, если там есть хоть какие-то пояснения, но есть и другая проблема. Наш продукт, например, находится в постоянном контакте с бизнесом. Логика его работы должна быть этому бизнесу понятна и где-то зафиксирована. При этом, естественно, в источнике, доступном бизнесу, она должна поддерживаться в актуальном состоянии. Даже если аккуратно описать все функции на понятном человеку языке, сгенерировать какой-нибудь [pydoc](https://docs.python.org/3/library/pydoc.html) или [sphinx](https://www.sphinx-doc.org/en/master/), то вряд ли вы порадуете менеджеров, если дадите им это почитать. Есть другой путь – написать менеджерам отдельное описание логики проекта. Но в таком случае при любом изменении логики (а они происходили постоянно), придётся поменять описание внутри кода, поменять описание функции, а затем ещё и переписать документацию для бизнеса. Вывод – хочется иметь такую документацию, которая была бы понятна и людям, работающим с кодом и бизнесу. А ещё неплохо бы тратить на неё не больше времени, чем на написание кода.\n",
    "\n",
    "* **Нелинейность кода.** С одной стороны код типа ```fun(fun1(fun2…))``` очень плохо воспринимается, а ситуации, когда он прямо таки необходим в ETL процесса встречаются крайне редко. С другой стороны, возвращаемся к вопросу документации и объяснения этой логики. Чем больше вложенности, тем сложнее восстановить логику в один связный текст.\n",
    "\n",
    "* **Недекомпозированность кода.** Работа датафреймами в pyspark ни в коем разе не останавливает полёт фантазии. Если у вас достаточно запала то написание функции можно не останавливать никогда. ```df = select.join.withColumn.withColumn.groupby.agg.select.withColumnRenamed...``` и дальше, дальше пока не оставят силы. Можно отдохнуть. А на следующий день ```df_new = df.select.join.withColumn...```. В общем, если без шуток, то я натыкался на файлы в несколько тысяч строк вот такого кода, в котором уже на 500 строчке забываешь, с чего, собственно, всё начиналось. Ещё одной большой проблемой длинного спарковского кода лично для меня является то, что очень быстро перестаёшь осознавать, какие вообще колонки есть в том или ином датафрейме.\n",
    "\n",
    "* **Сложность тестирования.** В основном, функции содержали обращения к таблицам базы внутри себя. В таких условиях очень тяжело их хоть как-то тестировать, так как тесты на больших таблицах занимают очень много времени, а при отсутствии доступа к базе вообще невозможны, а хотелось бы. Можно, конечно, собрать свою локальную песочницу и записать все тестовые таблицы туда. Но тесты частенько помогают понять логику кода, и хотелось бы иметь тестовые таблички всегда под рукой.\n",
    "\n",
    "Теперь, когда основные проблемы обозначены, можем перейти к примерам их решения. Далее я не хотел бы уделять время скучному разбору кода базовых классов, на которых строилось моё решение. Больше внимания я уделю основным концепциям с примерчиками использования, а сам код вы всегда сможете найти в [репозитории](https://github.com/n-surkov/PySparkPipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2071451",
   "metadata": {},
   "source": [
    "## Структура проекта\n",
    "\n",
    "Проекты разные, задачи разные, структуры разные. Говорить, что какая-то структура является универсальной смысла не имеет. В этом разделе просто будет приведён пример структуры, которая хорошо прижилась в нашем проекте, и в контексте которой я буду дальше вести повествование. Возьмём только минимально необходимое:\n",
    "\n",
    "```\n",
    "my_project:\n",
    "|--utils\n",
    "|  |--description_builder.py\n",
    "|--modules\n",
    "|  |--__init__.py\n",
    "|  |--module_base.py\n",
    "|  |--config_base.py\n",
    "|  |--module1\n",
    "|     |--__init__.py\n",
    "|     |--config.py\n",
    "|     |--module.py\n",
    "|     |--runner.py\n",
    "|--config.yml\n",
    "|--config_sources.yml\n",
    "```\n",
    "\n",
    "На самом верхнем уровне у нас лежат конфиги проекта. У нас было всего 2 файла, поэтому в корне они никому не мешались (`config.yml`, `config_sources.yml`). Если их больше, то, конечно, можно уместить их в отдельную директорию. Дальше директория со скриптами-утилитами (`utils`) и, собственно, дерево всех модулей проекта (`modules`).\n",
    "\n",
    "Модули проекта -- его логические части. Здесь изображена структура модулей одного уровня вложенности~~, потому что я устал рисовать~~ для простоты восприятия, но уровней может быть и больше. Главное, чтобы они были логически связаны. В каждом модуле лежит файл с конфигом этого модуля (`config.py`), файл в котором содержатся все вычисления (`module.py`) и скрипт для запуска (`runner.py`), в котором должно быть минимум кода. Файлы `module_base.py` и `config_base.py` содержат базовые классы, речь о которых пойдёт далее. Да, хранить базовые классы именно так не самое удачное решение, но оно работает, никого не беспокоит, поэтому пусть полежат здесь.\n",
    "\n",
    "Теперь, когда мы разобрались с основой структуры, можем переходить, собственно, к самому решению, которое, на мой взгляд, сделало жизнь в проекте намного проще."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a932f8e",
   "metadata": {},
   "source": [
    "## Конфиги проекта\n",
    "\n",
    "Начнём с конфигураций проекта. В большинстве случаев конфиги делятся на два типа:\n",
    "* **статические**, которые иногда меняются, но всё-таки крайне редко\n",
    "* **динамические**, которые может понадобиться менять чуть ли ни при каждом отдельном запуске скрипта. Обычно они же и передаются в качестве параметров в скриптах запуска\n",
    "\n",
    "Начнём со статических конфигов. Обычно это словарь, полученный при чтении yaml, json или подобного файла, где задана конфигурация проекта. В нашем случае это два yaml-файла. `config.yml` -- файл, содержащий параметры, общие для всего проекта. В  нашем проекте это были сопоставления городов и макрорегионов. `config_sources.yml` -- файл, содержащий список витрин, которые используются в проекте. Источники могут быть внешними (считаются другими продуктами) и внутренними (считаются внутри проекта). Также, под проект может быть выделено несколько баз (для прода, теста, бэкапов и т.д.) и всю эту информацию нужно как-то хранить. В своём проекте мы пришли к следующей схеме описания таблиц:\n",
    "\n",
    "```yaml\n",
    "# Внешние источники\n",
    "goods_tbl: 'edw_db.goods_table' # справочник товаров (ссылка)\n",
    "sales_tbl: 'edw_db.sales_table' # продажи магазинов (ссылка)\n",
    "\n",
    "# Базы данных продукта\n",
    "db_backups:\n",
    "    test: 'product_test_db' # база, в куторую сохраняются бэкапы расчётов\n",
    "    prod: 'product_prod_db' # база, в куторую сохраняются расчёты\n",
    "\n",
    "# Бэкапы\n",
    "backups: # витрины продукта, слепок которых делается в тестовую базу\n",
    "    product_table_1: # алиас таблицы\n",
    "        table_name: table_name_in_database # имя таблицы в базе\n",
    "        partitionedby: ['date', 'city'] # партиции\n",
    "```\n",
    "\n",
    "Сначала идёт перечисление внешних источников, которые мы только считываем. Затем список баз данных продукта, в нашем случае это продовая и тестовая базы. Затем идёт описание таблиц, расчёт которых производится в проекте. Здесь полезной дополнительной информацией будет список колонок, по которым производится партиционирование таблицы при записи.\n",
    "\n",
    "Если говорить о динамических конфигах, то у нас они всегда передавались через bash в качестве аргументов. В силу этого, каждый скрипт запуска начинается с добавления параметров в `argparse`, причём обычно, всё это полотно задаётся кописастом из загрузчика ранее написанного модуля. В силу этого, такие параметры тоже хотелось бы хранить в нашем конфиге.\n",
    "\n",
    "Ну и наконец, во время работы скриптов может понадобиться передавать некоторую информацию из функции в функцию. Конфиг, скорее всего будет тем объектом, который передаётся в каждую функцию и передавать параметры через него достаточно удобно.\n",
    "\n",
    "Итого, конфиг в проекте будет объектом класса, в котором существуют 4 атрибута:\n",
    "* `cfg` -- для статических конфигов из `config.yml` (считывается при инициализации объекта класса)\n",
    "* `cfg_sources` -- для хранения информации по базам данных из `config_sources.yml` (считывается при инициализации объекта класса)\n",
    "* `parameters` -- параметры, получающие данные при помощи `argparse`. Описание параметров, которые используются повсеместно (таких как, например, TEST -- флаг использования тестовой базы, или дата расчёта), можно задавать в `__init__` базового класса\n",
    "* `tmp` -- дополнительные данные, которые могут быть добавлены в конфиг в процессе вычислений\n",
    "\n",
    "А также несколько полезных функций:\n",
    "* `getitem`, `setitem` -- для того, чтобы к классу можно было обращаться, как к словарю\n",
    "* `print_description` -- для выведения описания всех конфигураций\n",
    "* `add_parameter`, `parse_arguments` -- для добавления параметров и парсинга их значений из bash\n",
    "* `get_table_link` -- функция, выдающая полный путь к таблице по её алиасу \n",
    "\n",
    "В конфиг можно также накидать и других функций, результат которых зависит от статических или динамических параметров, и которые используются повсеместно в проекте. Например, функцию, которая возвращает дату расчёта в нужном формате очень упрощает жизнь.\n",
    "\n",
    "Ниже пример использования конфига для получения ссылки на таблицу `product_table_1` из описания выше. В конфиге по умолчанию, добавлен параметр `TEST`, который определяет базу данных проекта для проведения вычислений. Также продемонстрировано, как работать с параметрами и временными значениями конфига."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2344cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test parameter 'False' table link is 'product_prod_db.table_name_in_database'\n",
      "For test parameter 'True' table link is 'product_test_db.table_name_in_database'\n",
      "---------------------------------------------------\n",
      "global_config contains:\n",
      "\n",
      "config_parameter_1 : 12\n",
      "config_parameter_2 : \n",
      "\tsubparam1 : param\n",
      "\tsubparam2 : noparam\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "sources_config contains:\n",
      "\n",
      "goods_tbl : edw_db.goods_table\n",
      "sales_tbl : edw_db.sales_table\n",
      "db_backups : \n",
      "\ttest : product_test_db\n",
      "\tprod : product_prod_db\n",
      "backups : \n",
      "\tproduct_table_1 : \n",
      "\t\ttable_name : table_name_in_database\n",
      "\t\tpartitionedby : ['date', 'city']\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "parameters contain:\n",
      "\n",
      "TEST : True\n",
      "calc_date : 20.08.2021\n",
      "logging_level : 10\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "temporary data contains:\n",
      "\n",
      "tmp_parameter : None\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from modules.config_base import ConfigBase\n",
    "# Пример вывода ссылки на таблицу\n",
    "config = ConfigBase()\n",
    "print(f\"For test parameter '{config['TEST']}' table link is '{config.get_table_link('product_table_1')}'\")\n",
    "config.update_config({'TEST': True})\n",
    "print(f\"For test parameter '{config['TEST']}' table link is '{config.get_table_link('product_table_1')}'\")\n",
    "\n",
    "# Добавление параметров\n",
    "config.add_parameter('logging_level', '--log_lev', type=int, default=10, nargs='?', help='Log level')\n",
    "config['tmp_parameter'] = None\n",
    "config.print_description()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4625b40",
   "metadata": {},
   "source": [
    "Даже такая простая функция, как возвращение пути к таблице по её алиасу, может существенно сократить количество однотипного кода в проекте. На функцию печати содержания конфига тоже рекомендую потратить время, это экономит много времени в дальнейшем, когда модулей в проекте становится больше, чем один.\n",
    "\n",
    "В заключении следует оговорить, что же тогда содержится в файлах `config.py` каждого отдельного модуля? В этих файлах происходит инициализация объекта базового класса `ConfigBase` и добавление в этот объект параметров, имеющих отношение к модулю (функция `add_parameter()`).\n",
    "\n",
    "Минимальная реализация базового класса под такой конфиг находится в файле `config_base.py` [репозитория](https://github.com/n-surkov/PySparkPipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb1b11",
   "metadata": {},
   "source": [
    "## Линеаризация структуры\n",
    "\n",
    "Иметь хороший конфиг, конечно, приятно, но [глобальных проблем](#Каким-же-должен-быть-код?) это не решает. Основной из них, на мой взгляд, является нелинейность написания кода. Для решения такого рода проблемы мне очень приглянулась идеология pipeline'ов, которые используются в pandas для построения алгоритмов обработки данных с последующим обучением моделек. В общем, каждый ETL процесс проекта мне хотелось видеть в виде последовательности шагов, выполняющихся один за другим. В моей терминологии я определил следующие два основных понятия:\n",
    "\n",
    "* Pipeline -- ETL процесс, состоящий из последовательного выполнения шагов, принимающий на вход таблицы из БД, результатом выполнения которого является новая таблица в БД (можно несколько, но желательно небольшое количество, иначе получается много намешанной логики)\n",
    "* Step -- Шаг вычислений, на вход принимает `spark.DataFrame` или обращается к базам напрямую. На выходе одна или несколько таблиц (в виде тех же `spark.DataFrame`), которые будут использоваться в дальнейших вычислениях \n",
    "\n",
    "Под каждый из этих пунктов был написан свой базовый класс, основные концепции которых я и попытаюсь продемонстрировать далее.\n",
    "\n",
    "### Step\n",
    "\n",
    "Пойдём от малого к большому и начнём с описания требований к шагу вычислений. Начнём с объёма -- шаг не должен быть большим. Обычно, в практике мы ориентировались на 100-150 строк спаркокода. Такое количество приемлемо, чтобы не потерять цепочку повествования даже при весьма заковыристой логике. Помимо этого, в каждом шаге хотелось бы иметь описание структур входных и выходных таблиц и в процессе вычислений сверять данные с этой структурой. Получаем следующие требования к объекту **Step**:\n",
    "\n",
    "* Так как мы целимся на нормальную документацию, каждый класс-шаг должен иметь качественное описание логики его вычислений\n",
    "* Должен иметь атрибуты, в которых хранятся описания схем входных и выходных таблиц\n",
    "* Если у нас есть описания таблиц, то пусть он проверяет вход на соответствие описанию перед началом вычислений \n",
    "* Должен иметь функцию, в которой, собственно, и будут производиться все основные вычисления\n",
    "* Должен иметь функцию `.run()`, которая будет возвращать список датафреймов, являющихся результатами вычислений. Отдельная функция тут нужна для того, чтобы проверять на соответствие описанию и выходные таблицы\n",
    "\n",
    "Написание кода в такой парадигме будет осуществляться следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fdea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.module_base import StepBase\n",
    "# Создание класса вычислений\n",
    "class SomeStep(StepBase):\n",
    "    \"\"\"\n",
    "    Название шага\n",
    "    \n",
    "    Описание вычислений класса в формате Markdown\n",
    "    \"\"\"\n",
    "    source_tables = {  # словарь описаний источников\n",
    "        'input_table_1': {  # имя, по которому можно будет обращаться к таблице при описании вычислений\n",
    "            'link': 'sales_tbl',  # ссылка на таблицу (можно алиасом из config_sources, как здесь), \n",
    "                                 # 'argument' если таблица не из БД\n",
    "            'description': 'Таблица для теста',  # описание таблицы\n",
    "            'columns': [  # список колонок\n",
    "                ('zrtpluid', 'string', 'plu', None), # имя, тип колонки в таблице, новое имя, новый тип\n",
    "                ('zprice', 'double', 'price', None),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_tables = {\n",
    "        'out_table_1': {\n",
    "            'link': None,  # ссылка на таблицу (можно алиасом из config_sources), для дальнейшего сохранения\n",
    "            'description': 'Таблица после теста',\n",
    "            'columns': [\n",
    "                ('plu', 'string'), # имя и тип колонки в выхоной таблице таблице\n",
    "                ('price', 'double'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def _calculations(self):\n",
    "        \"Вычисления класса, на выходе словарь в соответствии с self.output_tables\"\n",
    "        df = (\n",
    "            self.tables['input_table_1']\n",
    "            .withColumn('double_price', F.col('price') * 2)\n",
    "            .withColumn('price', F.round('price', 0))\n",
    "        )\n",
    "        return {'out_table_1': df}\n",
    "    \n",
    "# Инициализация объекта класса\n",
    "df = spark.createDataFrame([['555', 13.4]], ['zrtpluid', 'zprice'])\n",
    "argument_tables = {'input_table_1': df}  # Словарь с таблицами-аргументами\n",
    "step = SomeStep(spark, config, argument_tables, test=True, logger=logger)\n",
    "\n",
    "# Запуск вычислений\n",
    "result = step.run()\n",
    "result['out_table_1'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332163b",
   "metadata": {},
   "source": [
    "При этом на стадии инициализации объекта класса осуществляются:\n",
    "* проверки на совпадение названий и типов колонок таблиц на входе с описанием, заданным в `source_tables`\n",
    "* селект, переименование и изменение типов входных таблиц таким образом, чтобы они соответствовали описанию в `source_tables`\n",
    "* складирование таблиц в переменную класа `self.tables`\n",
    "\n",
    "На стадии запуска вычислений осуществается запуск функции `_calculations()` с последующей проверкой на то, что результат её вычислений соотносится с описанием `output_tables`.\n",
    "\n",
    "На первый взгляд, такой подход кажется избыточно сложным. В данном примере написание простейшей операции потребовало значительных затрат. Но когда дело касается более замысловатой логики, он себя более чем оправдывает, делая погружение в такой код много более приятным. Проверки на соответствие типов входных таблиц описанию ни раз спасали, как при дебаге нового кода, так и при рефакторинге старого. Несколько спорным является момент с добавлением в описание нового названия и типа колонок входных таблиц, ведь переименовывание колонок и смена типизации нужна не всегда. Тем не менее, по моему личному ощущению это намного приятнее, чем начинать каждую новую функцию с `.withColumnRenamed` и `cast`.\n",
    "\n",
    "Также хочется отдельно обговорить параметр `tets` при инициализации шага. Как видно в описании класса, ссылка на входную таблицу была задана алиасом из конфига. Если бы мы просто инициализировали шаг, то он бы начал стучаться в базу для того, чтобы получить схему таблицы и сравнить её с описанием. Но для тестирования нам бы хотелось просто передавать шагу на вход нужную нам таблицу. Для этого и вводится параметр `test`, который говорит шагу, что все источники переданы ему в качестве аргументов и в базы ломиться ему не нужно. \n",
    "\n",
    "### Pipeline\n",
    "\n",
    "С модулем Pipeline дела обстоят много проще. Основной принцип его действия:\n",
    "* создаём пустой словарь с таблицами `result`\n",
    "* начинаем последовательно запускать шаги, передавая каждому шагу на вход `result` и обогащая `result` рузультатом выполнения шага\n",
    "* после всех вычислений выбираем те таблицы, которые указаны в описании из `result`\n",
    "\n",
    "Для демонстрации написания кода пайплайна можно написать ещё один шаг, который будет производить те же самые вычисления, но название входной таблицы изменится на `out_table_1`, а выходной на `out_table_2`. Я вынесу за скобки этот копипаст и перейдём к примеру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81895763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.module_base import PipelineBase\n",
    "class Pipeline(PipelineBase):\n",
    "    \"\"\"\n",
    "    Название пайплана\n",
    "    \n",
    "    Общее описание всего пайплайна вычислений\n",
    "    \"\"\"\n",
    "    step_sequence = [\n",
    "        SomeStep,\n",
    "        AnotherStep\n",
    "    ]\n",
    "    \n",
    "    output_tables = {  # Аналогично описанию в Step\n",
    "        'out_table_2': {\n",
    "            'link': None,\n",
    "            'description': 'Финальная таблица',\n",
    "            'columns': [\n",
    "                ('plu', 'string'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Инициализация пайплайна\n",
    "pipeline = Pipeline(spark, config, test_arguments=argument_tables, logger=logger)\n",
    "\n",
    "# Запуск вычислений\n",
    "result = pipeline.run()\n",
    "result['out_table_2'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9800fd",
   "metadata": {},
   "source": [
    "При инициализации пайплацин сначала пробегается последовательно по всем шагам и проверяет возможность вычислений исодя из описаний входных-выходных таблиц каждого шага. Коротко говоря, проверяет, соответствует ли схема выходной таблице шага \"1\" схеме входной таблицы шага \"2\", если шаг \"2\" пользуется результатом вычислений шага \"1\". Для этого не требуется даже инициализация обектов каждого шага. Если со схемами всё в порядке, то начинаются последующие вычисления в соответствии с основынми принципами, которые были представлены в начале раздела.\n",
    "\n",
    "Важным моментом здесь является то, что в логику Pipeline закладывалось то, что он работает только с базами данных и не принимает ничего на вход. Но для теста это не удобно, поэтому нужно было предусмотреть подачу ему на вход <<игрушечных>> таблиц `test_arguments`. Если эти аргументы переданы, то при инициализации Pipeline будет запускать все свои шаги с параметром `test=True` и доступ к базам не потребуется.\n",
    "\n",
    "Описание класса Pipeline следует хранить в файле `module.py` для каждого конкретного модуля. Описание всех шагов может не уложиться в этот файл, поэтому можно растаскать их по другим файлам, но описание именно паплайна лучше хранить для каждого модуля в файле с одинаковым названием. Далее будет показано почему это особенно удобно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb06ed",
   "metadata": {},
   "source": [
    "## Вкусненькое\n",
    "\n",
    "Что же мы получаем, при использовании такой структуры?\n",
    "\n",
    "* Во-первых, использование качественного конфига в виде класса позволяет при любых вычислениях иметь его под рукой (он является обязательным аргументом при инициализации классов типа PipelineBase и передаётся во все его шаги) и делает доступ к параметрам простым и удобным. Особенно удобно, например, описать в конфиге функцию, позволяющую преобразовывать дату вычислений (которая скорее всего будет передаваться любому скрипту как параметр) в указанный формат. В моей практике одна такая функция позволила сокртить код не менее чем на сотню строк по всему проекту.\n",
    "\n",
    "* Во-вторых, при таком подходе сильно сокращается количество строк в файле `run.py`, который является криптом запуска модуля. Обычно, после всех импортов он выглядит как-то так:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    config.parse_arguments()\n",
    "    config.tune_logger(LOGGER)\n",
    "\n",
    "    spark = load_spark()\n",
    "\n",
    "    try:\n",
    "        pipeline = module.Pipeline(spark, config, logger=LOGGER)\n",
    "        result = pipeline.run()\n",
    "        pipeline.save_result_to_hive(save_mode='append')\n",
    "    finally:\n",
    "        spark.stop()\n",
    "```\n",
    "\n",
    "* В-третьих, на своём опыте заметил, что написание кода в концепции StepBase мотивирует людей к написанию функций разумного размера. Функции величиной в пару строк писать не хочется, потому что больше времени потратишь на описание входных-выходных таблиц. Функции величиной в 500-1000 строк тоже нет большого желания писать, так как придётся описывать их логику понятным для неподготовленного читателя языком в описании класса. Таким образом получается что средний пайплайн в нашем проекте состоял из 5-7 шагов с величиной каждого шага 100-200 строк кода. Погружаться в логику работы модулей при таких обстоятельствал стало не только возможно, но и достаточно приятно.\n",
    "\n",
    "* В-четвёртых, как мы уже успели убедиться, тестирование каждого шага является достаточно простым делом. Все таблицы, которые используются для вычислений можно достаточно просто передать на вход и не нужно ломать себе голову над тем, как вынести все инициализации `spark.table()` из тела функции.\n",
    "\n",
    "Но это ещё не всё. Одним из самых приятных моментов, что при таком подходе очень просто собирать описание работы пайплайнов! У нас для этого есть всё:\n",
    "* описание каждого шага на понятном языке (насколько это возможно)\n",
    "* описание входов и выходов каждого шага\n",
    "* краткое описание алгоритма в целом (его цели, время запуска и т.п. лучше как раз и указывать в описании класса Pipeline)\n",
    "\n",
    "Описание, сгенерированное автоматически из объекта класса Pipeline, описанием которого мы занимались на протяжении статьи будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8390e6",
   "metadata": {},
   "source": [
    "---\n",
    "**Название пайплана**\n",
    "\n",
    "\n",
    "Общее описание всего пайплайна вычислений\n",
    "\n",
    "<img src=\"pipeline.png\" width=300>\n",
    "\n",
    "**Модуль состоит из последовательного выполнения следующих шагов:**\n",
    "* **SomeStep** (Название шага):\n",
    "\n",
    "    Описание вычислений класса в формате Markdown\n",
    "\n",
    "  *Исходные таблицы:*\n",
    "\n",
    "\t* input_table_1 (argument) (Таблица для теста)\n",
    "\n",
    "  *Выходные таблицы:*\n",
    "\n",
    "\t* out_table_1 (Таблица после теста)\n",
    "\n",
    "* **AnotherStep** (Шаг, описание которого осталось за скобками):\n",
    "\n",
    "    Делает то же, что и предыдущий:\n",
    "    * удваивает цену, записывает результат в колонку `double_price`\n",
    "\n",
    "  *Исходные таблицы:*\n",
    "\n",
    "\t* out_table_1 (Результат вычислений шага SomeStep) (Таблица после теста)\n",
    "\n",
    "  *Выходные таблицы:*\n",
    "\n",
    "\t* out_table_2 (Таблица после второго теста)\n",
    "\n",
    "\n",
    "**Результатом выполнения алгоритма являются следующие таблицы:**\n",
    "* out_table_2 (Результат вычислений шага AnotherStep) (Финальная таблица):\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "  <tr>\n",
    "    <th>Название колонки</th><th>Формат</th>\n",
    "  </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "  <tr>\n",
    "    <td>plu</td><td>string</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>double_price</td><td>double</td>\n",
    "  </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac440451",
   "metadata": {},
   "source": [
    "Графы вычислений здесь отрисован с помощью утилиты [graphviz](https://graphviz.org). Сам код описания графа достаточно просто сгенерировать из пайплайна, аналогично документации. Печать графа в png можно осуществить при помощи утилиты `dot` на Linux иди Mac. Более того, если класс Pipeline будет приутствовать в файле `module.py` каждого модуля, то достаточно просто будет реализовать функцию, которая будет пробегать по всему проекту и в каждом модуле генерировать его описание в файл README.md. Затем это действие можно делегировать git CI и вообще забыть о том, что описание модулей нужно править где-то помимо кода проекта. Получаем качественное описание алгоритмов (с картинками, чтобы было нескучно читать), на поддержание которой в актуальном состоянии требуется минимум усилий. Профит!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd5efb",
   "metadata": {},
   "source": [
    "## Итого"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c950d5d",
   "metadata": {},
   "source": [
    "В данной статье у меня не было цели подробно разобрать концепцию пайплайнов при написании кода на спарке со всеми тонкостями её реализации. Целью являлось просто привести основные принципы, которые позволяют:\n",
    "\n",
    "* писать код понятным и последовательным образом\n",
    "* сократить время погружения в проект новых сотрудников\n",
    "* писать тесты без боли\n",
    "* составлять понятную лбычным людям документацию и легко поддерживать её в актуальном состоянии\n",
    "\n",
    "Я постарался оставить в статье минимум кода, лишь для демонстрации примеров, чтобы донести основные направления мысли, которыми я руководствовался, реализуя данный функционал. Не претендую на то, что такое решение является единственно верным, но как мне, так и моим коллегам работать в такой парадигме оказалось приятно. Надеюсь, поможет это и кому-нибудь ещё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9c892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8452316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeStep(StepBase):\n",
    "    \"\"\"\n",
    "    Название шага\n",
    "    \n",
    "    Описание вычислений класса в формате Markdown\n",
    "    \"\"\"\n",
    "    source_tables = {  # словарь описаний источников\n",
    "        'input_table_1': {  # имя, по которому можно будет обращаться к таблице при описании вычислений\n",
    "            'link': 'sales_tbl',  # ссылка на таблицу (можно алиасом из config_sources, как здесь), \n",
    "                                 # 'argument' если таблица не из БД\n",
    "            'description': 'Таблица для теста',  # описание таблицы\n",
    "            'columns': [  # список колонок\n",
    "                ('zrtpluid', 'string', 'plu', None), # имя, тип колонки в таблице, новое имя, новый тип\n",
    "                ('zprice', 'double', 'price', None),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_tables = {\n",
    "        'out_table_1': {\n",
    "            'link': None,  # ссылка на таблицу (можно алиасом из config_sources), для дальнейшего сохранения\n",
    "            'description': 'Таблица после теста',\n",
    "            'columns': [\n",
    "                ('plu', 'string'), # имя и тип колонки в выхоной таблице таблице\n",
    "                ('price', 'double'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def _calculations(self):\n",
    "        \"Вычисления класса, на выходе словарь в соответствии с self.output_tables\"\n",
    "        df = (\n",
    "            self.tables['input_table_1']\n",
    "            .withColumn('double_price', F.col('price') * 2)\n",
    "            .withColumn('price', F.round('price', 0))\n",
    "        )\n",
    "        return {'out_table_1': df}\n",
    "class AnotherStep(StepBase):\n",
    "    \"\"\"\n",
    "    Шаг, описание которого осталос за скобками\n",
    "    \n",
    "    Делает то же, что и предыдущий:\n",
    "    * удавивает цену, записывает результат в колонку `double_price`\n",
    "    \"\"\"\n",
    "    source_tables = {  # словарь описаний источников\n",
    "        'out_table_1': {  # имя, по которому можно будет обращаться к таблице при описании вычислений\n",
    "            'link': 'argument',  # ссылка на таблицу (можно алиасом из config_sources), \n",
    "                                 # 'argument' если таблица не из БД\n",
    "            'description': 'Таблица для теста',  # описание таблицы\n",
    "            'columns': [  # список колонок\n",
    "                ('plu', 'string', 'plu', None), # имя, тип колонки в таблице, новое имя, новый тип\n",
    "                ('price', 'double', 'price', None),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_tables = {\n",
    "        'out_table_2': {\n",
    "            'link': None,  # ссылка на таблицу (можно алиасом из config_sources), для дальнейшего сохранения\n",
    "            'description': 'Таблица после теста',\n",
    "            'columns': [\n",
    "                ('plu', 'string'), # имя и тип колонки в выхоной таблице таблице\n",
    "                ('price', 'double'),\n",
    "                ('double_price', 'double'),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def _calculations(self):\n",
    "        \"Вычисления класса, на выходе словарь в соответствии с self.output_tables\"\n",
    "        df = (\n",
    "            self.tables['out_table_1']\n",
    "            .withColumn('double_price', F.col('price') * 2)\n",
    "            .withColumn('price', F.round('price', 0))\n",
    "        )\n",
    "        return {'out_table_2': df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483e650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
